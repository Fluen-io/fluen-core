# src/fluen/llm_providers/ollama_provider.py

**Language:** Python  
**Last Modified:** 2024-11-12T20:10:07.635917

## Purpose

The code defines an asynchronous provider for interacting with an LLM service via HTTP API, with methods to generate text completions and obtain text embeddings.

## Public API

- `OllamaProvider` (exposure)
- `OllamaProvider.__init__` (exposure)
- `OllamaProvider.generate` (exposure)
- `OllamaProvider.get_embedding` (exposure)

## Dependencies

- `json` (external)
- `aiohttp` (external)
- `typing` (external)
- `.base_provider` (internal)

## Elements

### Class

#### `OllamaProvider`


**Purpose:** No purpose specified

**Documentation:**

No documentation available

### Method

#### `__init__`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `generate`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `get_embedding`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

### Variable

#### `api_base_url`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `max_retries`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `model`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `timeout`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available


[Back to Index](../README.md)
# src/fluen/llm_providers/ollama_provider.py

**Language:** Python  
**Last Modified:** 2024-11-03T10:05:48.728722

## Purpose

The primary purpose of this code is to interact with a language model API for generating text completions and obtaining text embeddings using asynchronous HTTP calls.

## Public API

- `OllamaProvider` (exposure)

## Dependencies

- `json` (external)
- `aiohttp` (external)
- `typing` (external)
- `base_provider.BaseLLMProvider` (external)

## Elements

### Class

#### `OllamaProvider`

**Scope:** global

**Purpose:** No purpose specified

**Documentation:**

No documentation available

### Method

#### `__init__`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `generate`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `get_embedding`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

### Variable

#### `api_base_url`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `max_retries`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `model`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available

#### `timeout`

**Scope:** OllamaProvider

**Purpose:** No purpose specified

**Documentation:**

No documentation available


[Back to Index](../README.md)